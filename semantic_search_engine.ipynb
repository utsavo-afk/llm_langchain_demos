{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a semantic search engine\n",
    "\n",
    "### Concepts\n",
    "\n",
    "This guide focuses on retrieval of text data. We will cover the following concepts:\n",
    "\n",
    "- Documents and document loaders;\n",
    "- Text splitters;\n",
    "- Embeddings;\n",
    "- Vector stores and retrievers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -qU langchain-community pypdf langchain-ollama langchain_unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents and Document Loaders\n",
    "\n",
    "LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n",
    "\n",
    "- page_content: a string representing the content;\n",
    "- metadata: a dict containing arbitrary metadata;\n",
    "- id: (optional) a string identifier for the document.\n",
    "\n",
    "The `metadata` attribute can capture information about the source of the document, its relationship to other documents, and other information.\n",
    "\n",
    "Note: that an individual `Document` object often represents a chunk of a larger document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# generate sample docs\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"source\": \"mammal-pets-doc\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"source\":\"mammal-pets-doc\"}\n",
    "    )\n",
    "]\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Preparing to split document for partition.\n",
      "INFO: Starting page number set to 1\n",
      "INFO: Allow failed set to 0\n",
      "INFO: Concurrency level set to 5\n",
      "INFO: Splitting pages 1 to 28 (28 total)\n",
      "INFO: Determined optimal split size of 6 pages.\n",
      "INFO: Partitioning 4 files with 6 page(s) each.\n",
      "INFO: Partitioning 1 file with 4 page(s).\n",
      "INFO: Partitioning set #1 (pages 1-6).\n",
      "INFO: Partitioning set #2 (pages 7-12).\n",
      "INFO: Partitioning set #3 (pages 13-18).\n",
      "INFO: Partitioning set #4 (pages 19-24).\n",
      "INFO: Partitioning set #5 (pages 25-28).\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n",
      "INFO: Successfully partitioned set #1, elements added to the final result.\n",
      "INFO: Successfully partitioned set #2, elements added to the final result.\n",
      "INFO: Successfully partitioned set #3, elements added to the final result.\n",
      "INFO: Successfully partitioned set #4, elements added to the final result.\n",
      "INFO: Successfully partitioned set #5, elements added to the final result.\n"
     ]
    }
   ],
   "source": [
    "# loading docs\n",
    "# from langchain_community.document_loaders import PyPDFLoader\n",
    "file_path='./data/zomato-q1fy25.pdf'\n",
    "# loader=PyPDFLoader(file_path=file_path) # PyPDFLoader loads one Document object per PDF page.\n",
    "# docs=loader.load()\n",
    "# len(docs) # 581\n",
    "\n",
    "# f\"{docs[0].page_content[:200]}\\n\" # page_content\n",
    "# f\"{docs[0].metadata}\" # metadata\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"UNSTRUCTURED_API_KEY\" not in os.environ:\n",
    "    os.environ[\"UNSTRUCTURED_API_KEY\"] = getpass.getpass(\"Unstructured API Key:\")\n",
    "\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "loader = UnstructuredLoader(\n",
    "    file_path=file_path,\n",
    "    strategy=\"hi_res\",\n",
    "    partition_via_api=True,\n",
    "    coordinates=True,\n",
    ")\n",
    "docs = []\n",
    "for doc in loader.lazy_load():\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting docs\n",
    "\"\"\"\n",
    "For both information retrieval and downstream question-answering \n",
    "purposes, a page may be too coarse a representation. Our goal \n",
    "in the end will be to retrieve Document objects that answer an \n",
    "input query, and further splitting our PDF will help ensure that \n",
    "the meanings of relevant portions of the document are not \"washed out\" \n",
    "by surrounding text.\n",
    "\n",
    "We can use text splitters for this purpose.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)\n",
    "\n",
    "all_splits=text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Generated vecs of length 3072\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings\n",
    "\"\"\"\n",
    "Vector search is a common way to store and search \n",
    "over unstructured data (such as unstructured text). \n",
    "The idea is to store numeric vectors that are associated \n",
    "with the text. Given a query, we can embed it as a vector \n",
    "of the same dimension and use vector similarity metrics \n",
    "(such as cosine similarity) to identify related text.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings=OllamaEmbeddings(model='llama3.2:latest')\n",
    "\n",
    "vec_1=embeddings.embed_query(all_splits[0].page_content)\n",
    "vec_2=embeddings.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(vec_1) == len(vec_2)\n",
    "f\"Generated vecs of length {len(vec_1)}\\n\"\n",
    "# vec_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "335"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorstore\n",
    "# Armed with a model for generating text embeddings, \n",
    "# we can next store them in a special data structure \n",
    "# that supports efficient similarity search.\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vec_store=InMemoryVectorStore(embedding=embeddings) # init vectorstore w/ embedding model\n",
    "ids = vec_store.add_documents(documents=all_splits)\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Doc:Deepinder: Today, Zomato and Blinkit are our two large consumer businesses and both of them serve customers' needs at home. However, we also have one of India’s largest ‘going-out’ businesses. Our dining-out business which helps our customers discover restaurants when they want to go out and dine at restaurants. This dining-out business is now operating at a run-rate of $500m+ annualised GOV and is already proﬁtable.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# querying\n",
    "\"\"\"\n",
    "Once we've instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying:\n",
    "\n",
    "Synchronously and asynchronously;\n",
    "By string query and by vector;\n",
    "With and without returning similarity scores;\n",
    "By similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results).\n",
    "\"\"\"\n",
    "\n",
    "results = vec_store.similarity_search(\n",
    "    \"25,000 unique SKUs\"\n",
    ")\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(id='f299860a-30e2-4cf6-8111-7002a485947e', metadata={'source': './data/zomato-q1fy25.pdf', 'coordinates': {'points': [[130.0000083333333, 1069.017333984375], [130.0000083333333, 1263.828125], [1512.6430320583331, 1263.828125], [1512.6430320583331, 1069.017333984375]], 'system': 'PixelSpace', 'layout_width': 1656, 'layout_height': 2339}, 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'parent_id': '9e23952d215bb433a3f9c13b20b971fa', 'filename': 'zomato-q1fy25.pdf', 'category': 'NarrativeText', 'element_id': '7fe99e400d7cff82bd3be308b7ccbde6', 'start_index': 0}, page_content=\"Deepinder: Today, Zomato and Blinkit are our two large consumer businesses and both of them serve customers' needs at home. However, we also have one of India’s largest ‘going-out’ businesses. Our dining-out business which helps our customers discover restaurants when they want to go out and dine at restaurants. This dining-out business is now operating at a run-rate of $500m+ annualised GOV and is already proﬁtable.\")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# different vectorsearch queries\n",
    "results = vec_store.similarity_search_with_score(\"Blinkit?\")\n",
    "doc, score = results[0]\n",
    "f\"Score: {score}\"\n",
    "f\"Doc:{doc.page_content}\"\n",
    "\n",
    "embedding = embeddings.embed_query(\"How is zomato performing against competitors?\")\n",
    "results = vec_store.similarity_search_by_vector(embedding)\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[Document(id='8489ad70-5e75-4b61-b291-2d893de36852', metadata={'source': './data/zomato-q1fy25.pdf', 'coordinates': {'points': [[122.37303161621094, 1104.950927734375], [122.37303161621094, 1204.6910400390625], [1449.06201171875, 1204.6910400390625], [1449.06201171875, 1104.950927734375]], 'system': 'PixelSpace', 'layout_width': 1656, 'layout_height': 2339}, 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 5, 'filename': 'zomato-q1fy25.pdf', 'category': 'Title', 'element_id': 'a3960af8dcbd496b2b99a7d0cec717c4', 'start_index': 0}, page_content='Q7. Can you share some data points that can help us appreciate assortment/ category expansion on the Blinkit platform over the past couple of years?')],\n",
       " [Document(id='f299860a-30e2-4cf6-8111-7002a485947e', metadata={'source': './data/zomato-q1fy25.pdf', 'coordinates': {'points': [[130.0000083333333, 1069.017333984375], [130.0000083333333, 1263.828125], [1512.6430320583331, 1263.828125], [1512.6430320583331, 1069.017333984375]], 'system': 'PixelSpace', 'layout_width': 1656, 'layout_height': 2339}, 'filetype': 'application/pdf', 'languages': ['eng'], 'page_number': 6, 'parent_id': '9e23952d215bb433a3f9c13b20b971fa', 'filename': 'zomato-q1fy25.pdf', 'category': 'NarrativeText', 'element_id': '7fe99e400d7cff82bd3be308b7ccbde6', 'start_index': 0}, page_content=\"Deepinder: Today, Zomato and Blinkit are our two large consumer businesses and both of them serve customers' needs at home. However, we also have one of India’s largest ‘going-out’ businesses. Our dining-out business which helps our customers discover restaurants when they want to go out and dine at restaurants. This dining-out business is now operating at a run-rate of $500m+ annualised GOV and is already proﬁtable.\")]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrievers\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def retriever(query: str)->List[Document]:\n",
    "    return vec_store.similarity_search(query)\n",
    "\n",
    "retriever.batch([\n",
    "    \"Can you share some data points that can help us appreciate assortment category expansion on the Blinkit platform over the past couple of years?\",\n",
    "    \"Any ESG updates?\"\n",
    "])\n",
    "\n",
    "# vectorestore implements as_retriever\n",
    "retriever = vec_store.as_retriever(\n",
    "    searchType='similarity',\n",
    "    search_kwargs={\"k\":1}\n",
    ")\n",
    "retriever.batch([\n",
    "    \"Can you share some data points that can help us appreciate assortment category expansion on the Blinkit platform over the past couple of years?\",\n",
    "    \"Any ESG updates?\"\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
