{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a simple LLM application with chat models and prompt templates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -qU langchain langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Salut le monde ! (Translation of \"Hello, world!\")', additional_kwargs={}, response_metadata={'model': 'llama3.2:latest', 'created_at': '2024-12-03T20:03:47.076086Z', 'done': True, 'done_reason': 'stop', 'total_duration': 893530083, 'load_duration': 30874583, 'prompt_eval_count': 37, 'prompt_eval_duration': 474000000, 'eval_count': 14, 'eval_duration': 384000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-660db390-7464-4ca8-a432-4a496e9cf39b-0', usage_metadata={'input_tokens': 37, 'output_tokens': 14, 'total_tokens': 51})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note that ChatModels receive message objects as input and \n",
    "generate message objects as output. In addition to text \n",
    "content, message objects convey conversational roles and \n",
    "hold important data, such as tool calls and token usage counts.\n",
    "\"\"\"\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English to French.\"),\n",
    "    HumanMessage(\"Hello, world!\")\n",
    "]\n",
    "model.invoke(messages) # recieves message as input, returns message as output\n",
    "\n",
    "\n",
    "# model.invoke(\"Hello\")\n",
    "# model.invoke([{\"role\":\"user\", \"content\":\"Hello\"}])\n",
    "# model.invoke([HumanMessage(\"Howdy?!\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour| le| monde| !||"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Because chat models are Runnables, they expose a standard \n",
    "interface that includes async and streaming modes of invocation. \n",
    "This allows us to stream individual tokens from a chat model\n",
    "\"\"\"\n",
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt templates\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template=\"Translate the following from English to {language}.\"\n",
    "\n",
    "prompt_template=ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template),(\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "prompt=prompt_template.invoke({\"language\":\"Italian\",\"text\":\"Hi!\"})\n",
    "# prompt\n",
    "# prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ciao! Come posso aiutarti oggi?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoke messages on model\n",
    "response=model.invoke(prompt)\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
